from typing import AsyncIterator, List, Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.language_models.chat_models import BaseChatModel

class ConclusionAgent:
    def __init__(self, model: BaseChatModel):
        self.model = model

    async def generate_report_stream(self, query: str, sections_content: List[Dict[str, Any]]) -> AsyncIterator[str]:
        """
        Aggregates section drafts into a final report, streaming XML output.
        sections_content: List of dicts with {'title': str, 'content': str, 'sources': List[dict], 'illustration': dict}
        
        Yields XML fragments:
        <section title="...">
            <text>...</text>
            <sources>
                <link url="..." title="..." />
            </sources>
        </section>
        """
        
        # Context buffer for deduplication
        report_context = ""

        # 1. Executive Summary (Generated by LLM)
        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert research synthesizer. Write a brief executive summary based on the following section headers. Do not include specific details that will be covered in the sections."),
            ("user", "Query: {query}\nSections: {sections}")
        ])
        section_titles = [s['title'] for s in sections_content]
        summary_chain = summary_prompt | self.model
        
        # We await the summary generation
        summary = await summary_chain.ainvoke({"query": query, "sections": ", ".join(section_titles)})
        summary_text = summary.content
        if isinstance(summary_text, list):
            summary_text = "".join([c.get("text", "") if isinstance(c, dict) else str(c) for c in summary_text])
        report_context += f"Executive Summary:\n{summary_text}\n\n"
        
        yield f'<section title="Executive Summary">\n<text>\n{summary_text}\n</text>\n</section>\n'
        
        # Refinement Chain Definition
        refine_prompt_template = """Here is what we have written so far:
<context>
{context}
</context>

Here is the draft for the next section ({title}):
<draft>
{draft}
</draft>

Here are the sources used in this draft:
{sources_formatted}

{illustration_instruction}

Task: Rewrite the draft to flow naturally from the context. REMOVE any information that has already been mentioned in the context to avoid repetition. Keep the unique details of this section.

Output Format: You MUST output valid XML with the following structure:
<section title="{title}">
<text>
...refined markdown content...
</text>
{illustration_placeholder}
<text>
...more content...
</text>
<sources>
<link url="..." title="..." />
</sources>
</section>

Do not output any text outside these tags. Use the provided sources to populate the links."""

        refine_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a research editor. You are assembling a final report section by section."),
            ("user", refine_prompt_template)
        ])
        refine_chain = refine_prompt | self.model

        # 2. Yield Sections with Context-Aware Refinement
        for section in sections_content:
            title = section['title']
            raw_content = section.get('content', '')
            sources = section.get('sources', [])
            illustration = section.get('illustration')
            
            # Format sources for the LLM
            sources_formatted = "\n".join([f"- Title: {s.get('title')}, URL: {s.get('url')}" for s in sources])
            
            illustration_instruction = ""
            illustration_placeholder_hint = ""
            if illustration:
                desc = "A generated interactive visualization."
                if illustration.get("type") == "code":
                    desc = "A generated interactive visualization (code provided)."
                elif illustration.get("type") == "image":
                    desc = f"An image titled '{illustration.get('alt', 'Image')}'."

                illustration_instruction = f"""
An illustration is available for this section: {desc}

DECISION REQUIRED:
1. Decide if this illustration helps explain the text.
2. If YES:
   - Place the tag `<illustration_placeholder/>` as a SIBLING to the `<text>` tags.
   - CRITICAL: Do NOT place `<illustration_placeholder/>` INSIDE `<text>...</text>`. It must be between text blocks.
   - Example Correct:
     <text>...content...</text>
     <illustration_placeholder/>
     <text>...more content...</text>
   - Example Wrong:
     <text>...content... <illustration_placeholder/> ...more content...</text>
3. If NO:
   - Do not include the placeholder tag.
"""
                illustration_placeholder_hint = "<illustration_placeholder/> (optional)"

            # Prepare illustration XML tag for replacement
            illustration_xml = ""
            if illustration:
                if illustration["type"] == "image":
                    illustration_xml = f'\n<image src="{illustration["url"]}" alt="{illustration.get("alt", "")}" />\n'
                elif illustration["type"] == "code":
                    illustration_xml = f'\n<code>{illustration["content"]}</code>\n'

            # Stream the refined section
            section_accumulator = ""
            buffer = ""
            illustration_inserted = False
            
            async for chunk in refine_chain.astream({
                "context": report_context, # Full context for coherence
                "title": title,
                "draft": raw_content,
                "sources_formatted": sources_formatted,
                "illustration_instruction": illustration_instruction,
                "illustration_placeholder": illustration_placeholder_hint
            }):
                content = chunk.content
                if isinstance(content, list):
                    content = "".join([c.get("text", "") if isinstance(c, dict) else str(c) for c in content])
                
                if not content: continue
                
                if illustration:
                    buffer += content
                    # Loop to handle multiple placeholders in one chunk or accumulated buffer
                    while "<illustration_placeholder/>" in buffer:
                        head, sep, tail = buffer.partition("<illustration_placeholder/>")
                        
                        # Yield the text before the placeholder
                        yield head
                        section_accumulator += head
                        
                        # Insert illustration ONLY ONCE
                        if not illustration_inserted:
                            yield illustration_xml
                            section_accumulator += illustration_xml
                            illustration_inserted = True
                        else:
                            # If duplicate placeholder, we just skip yielding the illustration again
                            # Essentially removing the duplicate placeholder from the output
                            pass
                            
                        # Update buffer to the rest of the string
                        buffer = tail
                else:
                    yield content
                    section_accumulator += content
            
            # Yield remaining buffer
            if buffer:
                yield buffer
                section_accumulator += buffer
            
            # Update context with the full section XML
            report_context += f"{section_accumulator}\n\n"
