from typing import AsyncIterator, List, Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.language_models.chat_models import BaseChatModel

class ConclusionAgent:
    def __init__(self, model: BaseChatModel):
        self.model = model

    async def generate_report_stream(self, query: str, sections_content: List[Dict[str, Any]]) -> AsyncIterator[str]:
        """
        Aggregates section drafts into a final report, streaming XML output.
        sections_content: List of dicts with {'title': str, 'content': str, 'sources': List[dict]}
        
        Yields XML fragments:
        <section title="...">
            <text>...</text>
            <sources>
                <link url="..." title="..." />
            </sources>
        </section>
        """
        
        # Context buffer for deduplication
        report_context = ""

        # 1. Executive Summary (Generated by LLM)
        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert research synthesizer. Write a brief executive summary based on the following section headers. Do not include specific details that will be covered in the sections."),
            ("user", "Query: {query}\nSections: {sections}")
        ])
        section_titles = [s['title'] for s in sections_content]
        summary_chain = summary_prompt | self.model
        
        # We await the summary generation
        summary = await summary_chain.ainvoke({"query": query, "sections": ", ".join(section_titles)})
        summary_text = summary.content
        report_context += f"Executive Summary:\n{summary_text}\n\n"
        
        yield f'<section title="Executive Summary">\n<text>\n{summary_text}\n</text>\n</section>\n'
        
        # Refinement Chain Definition
        refine_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a research editor. You are assembling a final report section by section."),
            ("user", """Here is what we have written so far:
<context>
{context}
</context>

Here is the draft for the next section ({title}):
<draft>
{draft}
</draft>

Here are the sources used in this draft:
{sources_formatted}

Task: Rewrite the draft to flow naturally from the context. REMOVE any information that has already been mentioned in the context to avoid repetition. Keep the unique details of this section.

Output Format: You MUST output valid XML with the following structure:
<section title="{title}">
<text>
...refined markdown content...
</text>
<sources>
<link url="..." title="..." />
</sources>
</section>

Do not output any text outside these tags. Use the provided sources to populate the links.""")
        ])
        refine_chain = refine_prompt | self.model

        # 2. Yield Sections with Context-Aware Refinement
        for section in sections_content:
            title = section['title']
            raw_content = section.get('content', '')
            sources = section.get('sources', [])
            
            # Format sources for the LLM
            sources_formatted = "\n".join([f"- Title: {s.get('title')}, URL: {s.get('url')}" for s in sources])
            
            # Stream the refined section
            section_accumulator = ""
            async for chunk in refine_chain.astream({
                "context": report_context[-3000:], # Context window
                "title": title,
                "draft": raw_content,
                "sources_formatted": sources_formatted
            }):
                content = chunk.content
                if content:
                    yield content
                    section_accumulator += content
            
            # Update context with the full section XML (or just the text part if we parsed it, but full XML is safer for context tracking)
            report_context += f"{section_accumulator}\n\n"
